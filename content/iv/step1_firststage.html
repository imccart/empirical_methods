---
title: "Testing the First Stage"
date: '2020-08-03'
output: 
  blogdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 3
bibliography: 'D:/CloudStation/Professional/Bibliography/BibTeX_Library.bib'
math: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The “first stage” is just a common term for a regression of the endogenous variable, <span class="math inline">\(x\)</span>, on your instrument, <span class="math inline">\(z\)</span> (and any other exogenous variables). If you have more than one endogeous variable, then you will have more than one first stage, but all of your instruments should be in each of those regressions. The intuition is simple, if you have a proposed instrument, it should at least be correlated with your endogenous variable of interest (after conditioning on all other exogenous variables already in the model). As with lots of econometrics, this sounds simple until it’s not. To preview some of this complexity, I’m only focusing on linear IV models here…the literature on non-linear IV models is less developed as far as I can tell.</p>
<p><img src="https://media.giphy.com/media/14xS2Ey9TXH0Gs/giphy.gif" /></p>
<div id="underidentification-versus-weak-instruments" class="section level2">
<h2>Underidentification versus weak instruments</h2>
<p>It is easy to confuse the question of underidentification versus weak instruments, but they are different issues. In the case of underidentification, we are explicitly interested in testing whether the partial correlation between the instrument(s) and the endogenous variable(s) is 0. In other words, is the instrument matrix of full rank? But it is possible to reject the null that the instruments and endogenous variables are uncorrelated, while still have very weak instruments. The literature therefore distinguishes between tests of 0 correlation versus tests that are robust to weak instruments (e.g., a test for which rejection of the null suggests that the instruments are sufficiently “strong”). Luckily, this distinction relates to the critical value to which you compare a given test statistic. In other words, a test of underidentification can use standard critical values, while a test allowing for weak instruments will tend to require slightly higher critical values (harder to reject).</p>
</div>
<div id="one-endogenous-variable" class="section level2">
<h2>One endogenous variable</h2>
<p>This is the simplest possible setup, and one in which you essentially focus on the significance (or, in the case of more than one instrument, joint significance) of your instruments in a regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(z\)</span> and all other exogenous variables. A standard t-test or F-test using traditional critical values would be a test of underidentification.</p>
<p>As is well-known, an F-stat of 10 is considered a decent rule-of-thumb for testing the first stage. This value was first proposed in <span class="citation">Staiger and Stock (1997)</span>, and it’s important because it’s larger than the standard critical values. The reason it’s larger is because the <span class="citation">Staiger and Stock (1997)</span> critical values allow for weak instruments. While the rule-of-thumb value of 10 is common and useful, the literature has probably taken this threshold a little too seriously (see Figure 1 in <span class="citation">Andrews, Stock, and Sun (2019)</span>, which shows a big spike at 10 in the empirical distribution of reported F-stats among papers published in the AER).</p>
<p>Another popular first-stage assessment is the partial <span class="math inline">\(R^{2}\)</span>, sometimes called Shea’s Partial <span class="math inline">\(R^{2}\)</span> <span class="citation">(Shea 1997)</span>. This is the <span class="math inline">\(R^{2}\)</span> of a regression of our endogenous variable on the instruments after partialling out the other exogenous variables. In other words, we regress our endogenous variable on our instrument(s), and we regress each of the exogenous covariates on our instrument(s). We then take the residual from each of those regressions and run another regression of the residualized endogenous variable against the residualized instruments. The <span class="math inline">\(R^{2}\)</span> of that regression is the partial <span class="math inline">\(R^{2}\)</span> because the influence of the the other exogenous variables has been removed. If the partial <span class="math inline">\(R^{2}\)</span> is “low”, then we should suspect that our instruments are weak. The problem is that there is no clear value for “low” or “high”, and the distribution of this statistic is nonstandard. I suspect that, for this reason, Shea’s Partial <span class="math inline">\(R^{2}\)</span> is not a commonly-reported measure of instrument strength in most published work.</p>
<p>Finally, <span class="citation">Olea and Pflueger (2013)</span> propose an “effective” F-stat that allows for heteroskedasticity. This hasn’t received much attention in the applied literature, but <span class="citation">Andrews, Stock, and Sun (2019)</span> encourages wider adoption of such a test. You can implement this in Stata with <code>weakivtest</code>. The effective F-stat is equivalent to a robust F-stat (i.e., the usual first-stage F-stat using robust standard errors) in the case of a single instrument. But with multiple instruments and one endogenous variable, the effective F-stat looks to be the best option for assessing the strength of your instruments.</p>
</div>
<div id="multiple-endogenous-variables" class="section level2">
<h2>Multiple endogenous variables</h2>
<p>With more than one endogenous variable, <span class="citation">Stock and Yogo (2005)</span> develops critical values based on the Cragg-Donald statistic <span class="citation">(Cragg and Donald 1993)</span>. A statistic similar to Cragg-Donald is the Anderson LM statistic, both of which are readily reported in Stata’s <code>ivreg2</code> command. Unfortunately, both statistics require the assumption of homoskedastic errors. Shea’s Partial <span class="math inline">\(R^{2}\)</span> also isn’t particularly informative in the case of multiple endogenous variables, again a potential reason for its limited use in practice.</p>
<p>In the more realistic heteroskedastic setting, the appropriate tests are more complicated. The Kleibergen-Paap rank-based (<em>rk</em>) statistic can be used to implement a Lagrange-Multiplier (LM) test that is a robust version of the Cragg-Donald statistic <span class="citation">(Kleibergen and Paap 2006)</span>; however, the Kleibergen-Paap LM test is purely an “underidentification” test, and it is possible to reject the null while still having very weak correlation between your instruments and endogenous variables. For inference that is robust to weak instruments and heteroskedasticity, we can employ the Kleibergen-Paap <em>rk</em> Wald test with the critical values from <span class="citation">Stock and Yogo (2005)</span>. This is the approach recommended in <span class="citation">Baum, Schaffer, and Stillman (2007)</span>, although this approach is not formalized.</p>
<p>Note that the Cragg-Donald statistic or the Kleibergen-Paap <em>rk</em> statistic each provide a single test statistic for all of the instruments and all of the endogenous variables. But what if you have more than one endogenous variable and just one of the IVs is predominatly driving the identification in all cases? In that case, it can be good to look at tests separately for each endogenous variable. <span class="citation">Angrist and Pischke (2009)</span> and <span class="citation">Sanderson and Windmeijer (2016)</span> provide such an approach.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-andrews2019">
<p>Andrews, Isaiah, James H Stock, and Liyang Sun. 2019. “Weak Instruments in Instrumental Variables Regression: Theory and Practice.” <em>Annual Review of Economics</em> 11: 727–53.</p>
</div>
<div id="ref-angrist2009">
<p>Angrist, J., and J. Pischke. 2009. <em>Mostly Harmless Econometrics</em>. Princeton, NJ: Princeton University Press.</p>
</div>
<div id="ref-baum2007">
<p>Baum, Christopher F, Mark E Schaffer, and Steven Stillman. 2007. “Enhanced Routines for Instrumental Variables/Generalized Method of Moments Estimation and Testing.” <em>The Stata Journal</em> 7 (4): 465–506.</p>
</div>
<div id="ref-cragg1993">
<p>Cragg, John G, and Stephen G Donald. 1993. “Testing Identifiability and Specification in Instrumental Variable Models.” <em>Econometric Theory</em>, 222–40.</p>
</div>
<div id="ref-kleibergen2006">
<p>Kleibergen, Frank, and Richard Paap. 2006. “Generalized Reduced Rank Tests Using the Singular Value Decomposition.” <em>Journal of Econometrics</em> 133 (1): 97–126.</p>
</div>
<div id="ref-olea2013">
<p>Olea, José Luis Montiel, and Carolin Pflueger. 2013. “A Robust Test for Weak Instruments.” <em>Journal of Business &amp; Economic Statistics</em> 31 (3): 358–69.</p>
</div>
<div id="ref-sanderson2016">
<p>Sanderson, Eleanor, and Frank Windmeijer. 2016. “A Weak Instrument F-Test in Linear Iv Models with Multiple Endogenous Variables.” <em>Journal of Econometrics</em> 190 (2): 212–21.</p>
</div>
<div id="ref-shea1997">
<p>Shea, John. 1997. “Instrument Relevance in Multivariate Linear Models: A Simple Measure.” <em>The Review of Economics and Statistics</em> 79 (2): 348–52.</p>
</div>
<div id="ref-staiger1997">
<p>Staiger, Douglas, and James H Stock. 1997. “Instrumental Variables Regression with Weak Instruments.” <em>Econometrica</em> 65 (3): 557–86.</p>
</div>
<div id="ref-stock2005">
<p>Stock, James H, and Motohiro Yogo. 2005. “Testing for Weak Instruments in Linear Iv Regression.” In <em>Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg</em>. Cambridge University Press.</p>
</div>
</div>
</div>
