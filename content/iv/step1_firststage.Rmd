---
title: "Testing the First Stage"
date: '2020-08-03'
output: 
  blogdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 3
bibliography: 'D:/CloudStation/Professional/Bibliography/BibTeX_Library.bib'
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The "first stage" is just a common term for a regression of the endogenous variable, $x$, on your instrument, $z$ (and any other exogenous variables). If you have more than one endogeous variable, then you will have more than one first stage, but all of your instruments should be in each of those regressions. The intuition is simple, if you have a proposed instrument, it should at least be correlated with your endogenous variable of interest (after conditioning on all other exogenous variables already in the model). As with lots of econometrics, this sounds simple until it's not. To preview some of this complexity, I'm only focusing on linear IV models here...the literature on non-linear IV models is less developed as far as I can tell.

![](https://media.giphy.com/media/14xS2Ey9TXH0Gs/giphy.gif)


## Underidentification versus weak instruments
It is easy to confuse the question of underidentification versus weak instruments, but they are different issues. In the case of underidentification, we are explicitly interested in testing whether the partial correlation between the instrument(s) and the endogenous variable(s) is 0. In other words, is the instrument matrix of full rank? But it is possible to reject the null that the instruments and endogenous variables are uncorrelated, while still have very weak instruments. The literature therefore distinguishes between tests of 0 correlation versus tests that are robust to weak instruments (e.g., a test for which rejection of the null suggests that the instruments are sufficiently "strong"). Luckily, this distinction relates to the critical value to which you compare a given test statistic. In other words, a test of underidentification can use standard critical values, while a test allowing for weak instruments will tend to require slightly higher critical values (harder to reject).

## One endogenous variable
This is the simplest possible setup, and one in which you essentially focus on the significance (or, in the case of more than one instrument, joint significance) of your instruments in a regression of $x$ on $z$ and all other exogenous variables. A standard t-test or $F$-test using traditional critical values would be a test of underidentification.

As is well-known, an $F$ statistic of 10 is considered a decent rule-of-thumb for testing the first stage. This value was first proposed in @staiger1997, and it's important because it's larger than the standard critical values. The reason it's larger is because the @staiger1997 critical values allow for weak instruments. While the rule-of-thumb value of 10 is common and useful, the literature has probably taken this threshold a little too seriously (see Figure 1 in @andrews2019, which shows a big spike at 10 in the empirical distribution of reported F-stats among papers published in the AER). 

Another popular first-stage assessment is the partial $R^{2}$, sometimes called Shea's Partial $R^{2}$ [@shea1997]. This is the $R^{2}$ of a regression of our endogenous variable on the instruments after partialling out the other exogenous variables. In other words, we regress our endogenous variable on our instrument(s), and we regress each of the exogenous covariates on our instrument(s). We then take the residual from each of those regressions and run another regression of the residualized endogenous variable against the residualized instruments. The $R^{2}$ of that regression is the partial $R^{2}$ because the influence of the the other exogenous variables has been removed. If the partial $R^{2}$ is "low", then we should suspect that our instruments are weak. The problem is that there is no clear value for "low" or "high", and the distribution of this statistic is nonstandard. I suspect that, for this reason, Shea's Partial $R^{2}$ is not a commonly-reported measure of instrument strength in most published work.

Finally, @olea2013 propose an "effective" $F$-stat that allows for heteroskedasticity. This hasn't received much attention in the applied literature, but @andrews2019 encourages wider adoption of such a test. You can implement this in Stata with `weakivtest`. The effective $F$-stat is equivalent to a robust $F$-stat (i.e., the usual first-stage $F$-stat using robust standard errors) in the case of a single instrument. But with multiple instruments and one endogenous variable, the effective $F$-stat looks to the best option for assessing the strength of your instruments.


## Multiple endogenous variables
With more than one endogenous variable, @stock2005 develops critical values based on the Cragg-Donald statistic [@cragg1993]. A statistic similar to Cragg-Donald is the Anderson LM statistic, both of which are readily reported in Stata's `ivreg2` command. Unfortunately, both statistics require the assumption of homoskedastic errors. Shea's Partial $R^{2}$ also isn't particularly informative in the case of multiple endogenous variables, again a potential reason for its limited use in practice.

In the more realistic heteroskedastic setting, the appropriate tests are more complicated. The Kleibergen-Paap rank-based (*rk*) statistic can be used to implement a Lagrange-Multiplier (LM) test that is a robust version of the Cragg-Donald statistic [@kleibergen2006]; however, the Kleibergen-Paap *rk* statistic is purely an "underidentification" test, and it is possible to reject the null while still having very weak correlation between your instruments and endogenous variables. For inference that is robust to weak instruments and heteroskedasticity, we can employ the Kleibergen-Paap *rk* Wald test with the critical values from @stock2005. This is the approach recommended in @baum2007, although this approach is not formalized.

Note that the Cragg-Donald statistic or the Kleibergen-Paap *rk* statistic each provide a single test statistic for all of the instruments and all of the endogenous variables. But what if you have more than one endogenous variable and just one of the IVs is predominatly driving the identification in all cases? In that case, it can be good to look at tests separately for each endogenous variable. @angrist2009 and @sanderson2016 provide such an approach.
