---
title: "How big is the problem?"
date: '2020-08-03'
output: 
  blogdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 3
bibliography: 'D:/CloudStation/Professional/Bibliography/BibTeX_Library.bib'
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

It would be great if we could test for whether we need IV or not. But unfortunately, I've already oversold this section. There is no magic "test" for whether you need IV.

![](https://media.giphy.com/media/oXB0K4oFw3fck/giphy.gif)

But all is not lost. There are a couple of things that we can try. While these approaches won't definitively point us to IV, they will at least inform us as to the extent of the potential endogeneity problem and tell us how different some IV results might be relative to OLS (assuming we have some decent instruments already).

![](https://media.giphy.com/media/ZExucn4EDMUtX0p9dt/giphy.gif)


## Coefficient Stability 

A good first step is to quantify just how bad the endogeneity problem would need to be in order to negate your initial findings (say, from good ol' OLS). @oster2019 can help us here. Here's the idea...

Lots of applied researchers assess "coefficient stability" by including different sets of control variables that are intended to proxy for some potentially important unobserved factor. This is not informative of omitted variables bias if the existing controls already do a very poor job of explaining the outcome. As Prof. Oster notes, "Omitted variable bias is proportional to coefficient movements, but only if such movements are scaled by the change in R-squared when controls are included."

Extending the work of @altonji2005, @oster2019 lays out a scenario in which we can fully decompose our outcome of interest into a treatment effect (denoted $\beta$), observed controls (denoted by $W_{1}$), unobserved controls (denoted by $W_{2}$), and some iid error term. Denote by $X$ the treatment variable, such that $$Y = \beta X + W_{1} + W_{2} + \epsilon.$$ We then need to consider values (or a range of values) for two key objects. 

1. What is the maximum $R^2$ value we could obtain if we observed $W_{2}$? Let's call this $R_{\text{max}}^{2}$. If we think the outcome is fully deterministic if we were to observe all relevant variables, then $R_{\text{max}}^{2}=1$, but we could consider smaller values as well.

2. What is the degree of selection on observed variables relative to unobserved variables? We can denote this value as $\delta$, and define $\delta$ as the value such that: $$\delta \times \frac{Cov(W_{1},X)}{Var(W_{1})} = \frac{Cov(W_{2},X)}{Var(W_{2})}.$$

We then need to define a few objects that we can directly estimate with the data:

1. Denote by $R^{2}_{X}$ the $R^{2}$ from a regression of $Y$ on treatment (and only treatment, no covariates). Similarly denote by $\hat{\beta}_{X}$ the value of $\beta$ estimated from that regression. 

2. Denote by $R^{2}_{X,W_{1}}$ the $R^{2}$ from a regression of $Y$ on treatment and observed controls. Again, denote the estimated value of $\beta$ from this regression as $\hat{\beta}_{X, W_{1}}$. 

Under the assumption that the relative size of coefficients from a regression of $Y$ on $X$ and observed variables are equal to those from a regression of $X$ and the observed variables, @oster2019 then shows that the true coefficient of interest ($\beta$ from the full regression) converges to the following:

$$\beta^{*} \approx \hat{\beta}_{X,W_{1}} - \delta \times \left[\hat{\beta}_{X} - \hat{\beta}_{X,W_{1}}\right] \times \frac{R_{max}^{2} - R_{X,W_{1}}^{2}}{R_{X,W_{1}}^{2} - R_{X}^{2}} \xrightarrow{p} \beta.$$

If we relax the assumption of equal "relative contributions" between the observed covariates and $Y$ versus the observed covariates and $X$, then the results are a little more complicated. In that case, @oster2019 shows that $$\beta^{*} = \hat{\beta}_{X,W_{1}} - \nu_{1} \xrightarrow{p} \beta,$$ or $$\beta^{*} \in \left\{ \hat{\beta}_{X,W_{1}} - \nu_{1}, \hat{\beta}_{X,W_{1}} - \nu_{2}, \hat{\beta}_{X,W_{1}} - \nu_{3} \right\},$$
where $\nu_{1}$, $\nu_{2}$, and $\nu_{3}$ are roots of a cubic function, $f(\nu)$, derived in the paper. In the case of more than one root, then one element of $\beta^{*}$ converges in probability to $\beta$. If $\delta=1$, then some additional simplifications can be made, but the point is that we now have an expression for the bias as a function of $\delta$ and $R^{2}_{max}$. 

So what do we gain from all of this? Well, @oster2019 shows that we can also work backwards and find the value of $\delta$ such that $\beta=0$. In other words, say we estimate using OLS some effect, $\hat{\beta}_{X, W_{1}}$. How big must the role of selection on unobservables be in order to completely overpower our estimate such that the true effect is actually 0? 

Another approach is to consider a range of $R^{2}_{max}$ and $\delta$ to bound the estimated treatment effect. Using $\delta=1$ as an upper bound for $\delta$ (i.e., observables are at least as important as the unobservables), and $\bar{R}^{2}_{max}$ as an upper bound for $R^{2}_{max}$, then the bounds on $\beta^{*}$ are $\left[ \hat{\beta}_{X,W_{1}}, \beta^{*}(\bar{R}^{2}_{max}, 1) \right]$.

Finally, @oster2019 suggests setting $\delta=1$ and identifying the value of $R^{2}_{max}$ for which $\beta=0$. This would tell us how much of the variation in $Y$ would need to be explained by unobservables in order for the true effect to be null (given our estimate, $\hat{\beta}_{X,W_{1}}$.

There is also a Stata command, `psacalc`, to do these calculations for us!  

## IV vs OLS

An easy way to assess the need for IV is to simply test whether your IV results are sufficiently different from OLS. That's the spirit of the Hausman test. The original test introduced in @hausman1978 is not specific to endogeneity...it's a more general misspecification test, comparing the estimates from one estimator (that is efficient under the null) to that of another estimator that is consistent but inefficient under the null. The test in the context of IV is also referred to as the Durbin-Wu-Hausman test, due to the series of papers pre-dating @hausman1978, including @durbin1954, @wu1973, and @wu1974.

This test is easily implemented as an "artificial" or "augmented" regression. Denoting our outcome by $y$, our instruments by $z$, our endogeous variables by $x_{1}$, and other exogenous variables by $x_{2}$, we first regress each of the variables in $x_{1}$ on $x_{2}$ and $z$. Then we take the residuals from those regressions, denoted $\hat{v}$, and include them in the standard OLS regression of $y$ on $x_{1}$, $x_{2}$, and $\hat{v}$.

The biggest barrier to this test in practice is that it assumes we have a valid and strong set of instruments, $z$. Since that's usually the biggest barrier to causal inference with IV, it becomes a major practical problem. For example, if you reject the null and conclude that estimates from OLS and IV are statistically different, can you be sure that the difference is "real" and not a statistical artifact of weak or invalid instruments? The whole process becomes pretty circular.


![](https://media.giphy.com/media/dyGiQTZrrASFWp9qP8/source.gif)


# References