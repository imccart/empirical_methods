---
title: "Pre-testing the Need for IV"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 3
bibliography: 'D:/CloudStation/Professional/Bibliography/BibTeX_Library.bib'    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


I've already oversold this section. There is no magic "test" for whether you need IV.

![](https://media.giphy.com/media/oXB0K4oFw3fck/giphy.gif)

But all is not lost. There are a couple of things that we can try. While these approaches won't definitively point us to IV, they will at least inform us as to the extent of the potential endogeneity problem and tell us how different some IV results might be relative to OLS (assuming we have some decent instruments already).

![](https://media.giphy.com/media/ZExucn4EDMUtX0p9dt/giphy.gif)


## Coefficient Stability 

A good first step is to quantify just how bad the endogeneity problem would need to be in order to negate your initial findings (say, from good ol' OLS). @oster2017 can help us here. Here's the idea...

Lots of applied researchers assess "coefficient stability" by including different sets of control variables that are intended to proxy for some potentially important unobserved factor. This is not informative of omitted variables bias if the existing controls already do a very poor job of explaining the outcome. As Prof. Oster notes, "Omitted variable bias is proportional to coefficient movements, but only if such movements are scaled by the change in R-squared when controls are included."

Extending the work of @altonji2015, @oster2017 lays out a scenario in which we can fully decompose our outcome of interest into a treatment effect (denoted $\beta$), observed controls (denoted by $W_{1}$), unobserved controls (denoted by $W_{2}$), and some iid error term. Denote by $X$ the treatment variable, such that $$Y = \beta X + W_{1} + W_{2} + \epsilon.$$ We then need to consider values (or a range of values) for two key objects. 

1. What is the maximum $R^2$ value we could obtain if we observed $W_{2}$? Let's call this $R_{\text{max}}^{2}$. If we think the outcome is fully deterministic if we were to observe all relevant variables, then $R_{\text{max}}^{2}=1$, but we could consider smaller values as well.

2. What is the degree of selection on observed variables relative to unobserved variables? We can denote this value as $\delta$, and define $\delta$ as the value such that: $$\delta \times \frac{Cov(W_{1},X)}{Var(W_{1})} = \frac{Cov(W_{2},X)}{Var(W_{2})}.$$

We then need to define a few objects that we can directly estimate with the data:

1. Denote by $R^{2}_{X}$ the $R^{2}$ from a regression of $Y$ on treatment (and only treatment, no covariates). Similarly denote by $\hat{\beta}_{X}$ the value of $\beta$ estimated from that regression. 

2. Denote by $R^{2}_{X,W_{1}}$ the $R^{2}$ from a regression of $Y$ on treatment and observed controls. Again, denote the estimated value of $\beta$ from this regression as $\hat{\beta}_{X, W_{1}}$. 

Under the assumption that the relative size of coefficients from a regression of $Y$ on $X$ and observed variables are equal to those from a regression of $X$ and the observed variables, @oster2017 then shows that the true coefficient of interest ($\beta$ from the full regression) converges to the following:

$$\beta^{*} \approx \hat{\beta}_{X,W_{1}} - \delta \times \left[\hat{\beta}_{X} - \hat{\beta}_{X,W_{1}}\right] \times \frac{R_{max}^{2} - R_{X,W_{1}}^{2}}{R_{X,W_{1}}^{2} - R_{X}^{2}}.$$

If we relax the assumption of equal "relative contributions" between the observed covariates and $Y$ versus the observed covariates and $X$, then the results are a little more complicated. In that case, @oster2017 shows that $$\beta^{*} = \hat{\beta}_{X,W_{1}} - \nu_{1},$$ or $$\beta^{*} \in \left\{ \hat{\beta}_{X,W_{1}} - \nu_{1}, \hat{\beta}_{X,W_{1}} - \nu_{2}, \hat{\beta}_{X,W_{1}} - \nu_{3} \right\}.$$

# References