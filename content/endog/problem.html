---
title: "How big is the problem?"
date: '2020-08-03'
output: 
  blogdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 3
bibliography: '../../BibTeX_Library.bib'
math: true
---



<p>It would be great if we could test for whether we really had an endogeneity problem or not. But alas, that’s just not in the cards. Instead, a good starting point is to see “how much” of an endogeneity problem we’d have to have to overturn our current results.</p>
<p><span class="citation">Oster (2019)</span> can help us here. Here’s the idea. Lots of applied researchers assess “coefficient stability” by including different sets of control variables that are intended to proxy for some potentially important unobserved factor. This is not informative of omitted variables bias if the existing controls already do a very poor job of explaining the outcome. As Prof. Oster notes, “Omitted variable bias is proportional to coefficient movements, but only if such movements are scaled by the change in R-squared when controls are included.”</p>
<p>Extending the work of <span class="citation">Altonji, Elder, and Taber (2005)</span>, <span class="citation">Oster (2019)</span> lays out a scenario in which we can fully decompose our outcome of interest into a treatment effect (denoted <span class="math inline">\(\beta\)</span>), observed controls (denoted by <span class="math inline">\(W_{1}\)</span>), unobserved controls (denoted by <span class="math inline">\(W_{2}\)</span>), and some iid error term. Denote by <span class="math inline">\(X\)</span> the treatment variable, such that <span class="math display">\[Y = \beta X + W_{1} + W_{2} + \epsilon.\]</span> We then need to consider values (or a range of values) for two key objects.</p>
<ol style="list-style-type: decimal">
<li><p>What is the maximum <span class="math inline">\(R^2\)</span> value we could obtain if we observed <span class="math inline">\(W_{2}\)</span>? Let’s call this <span class="math inline">\(R_{\text{max}}^{2}\)</span>. If we think the outcome is fully deterministic if we were to observe all relevant variables, then <span class="math inline">\(R_{\text{max}}^{2}=1\)</span>, but we could consider smaller values as well.</p></li>
<li><p>What is the degree of selection on observed variables relative to unobserved variables? We can denote this value as <span class="math inline">\(\delta\)</span>, and define <span class="math inline">\(\delta\)</span> as the value such that: <span class="math display">\[\delta \times \frac{Cov(W_{1},X)}{Var(W_{1})} = \frac{Cov(W_{2},X)}{Var(W_{2})}.\]</span></p></li>
</ol>
<p>We then need to define a few objects that we can directly estimate with the data:</p>
<ol style="list-style-type: decimal">
<li><p>Denote by <span class="math inline">\(R^{2}_{X}\)</span> the <span class="math inline">\(R^{2}\)</span> from a regression of <span class="math inline">\(Y\)</span> on treatment (and only treatment, no covariates). Similarly denote by <span class="math inline">\(\hat{\beta}_{X}\)</span> the value of <span class="math inline">\(\beta\)</span> estimated from that regression.</p></li>
<li><p>Denote by <span class="math inline">\(R^{2}_{X,W_{1}}\)</span> the <span class="math inline">\(R^{2}\)</span> from a regression of <span class="math inline">\(Y\)</span> on treatment and observed controls. Again, denote the estimated value of <span class="math inline">\(\beta\)</span> from this regression as <span class="math inline">\(\hat{\beta}_{X, W_{1}}\)</span>.</p></li>
</ol>
<p>Under the assumption that the relative size of coefficients from a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and observed variables are equal to those from a regression of <span class="math inline">\(X\)</span> and the observed variables, <span class="citation">Oster (2019)</span> then shows that the true coefficient of interest (<span class="math inline">\(\beta\)</span> from the full regression) converges to the following:</p>
<p><span class="math display">\[\beta^{*} \approx \hat{\beta}_{X,W_{1}} - \delta \times \left[\hat{\beta}_{X} - \hat{\beta}_{X,W_{1}}\right] \times \frac{R_{max}^{2} - R_{X,W_{1}}^{2}}{R_{X,W_{1}}^{2} - R_{X}^{2}} \xrightarrow{p} \beta.\]</span></p>
<p>If we relax the assumption of equal “relative contributions” between the observed covariates and <span class="math inline">\(Y\)</span> versus the observed covariates and <span class="math inline">\(X\)</span>, then the results are a little more complicated. In that case, <span class="citation">Oster (2019)</span> shows that <span class="math display">\[\beta^{*} = \hat{\beta}_{X,W_{1}} - \nu_{1} \xrightarrow{p} \beta,\]</span> or <span class="math display">\[\beta^{*} \in \left\{ \hat{\beta}_{X,W_{1}} - \nu_{1}, \hat{\beta}_{X,W_{1}} - \nu_{2}, \hat{\beta}_{X,W_{1}} - \nu_{3} \right\},\]</span>
where <span class="math inline">\(\nu_{1}\)</span>, <span class="math inline">\(\nu_{2}\)</span>, and <span class="math inline">\(\nu_{3}\)</span> are roots of a cubic function, <span class="math inline">\(f(\nu)\)</span>, derived in the paper. In the case of more than one root, then one element of <span class="math inline">\(\beta^{*}\)</span> converges in probability to <span class="math inline">\(\beta\)</span>. If <span class="math inline">\(\delta=1\)</span>, then some additional simplifications can be made, but the point is that we now have an expression for the bias as a function of <span class="math inline">\(\delta\)</span> and <span class="math inline">\(R^{2}_{max}\)</span>.</p>
<p>So what do we gain from all of this? Well, <span class="citation">Oster (2019)</span> shows that we can also work backwards and find the value of <span class="math inline">\(\delta\)</span> such that <span class="math inline">\(\beta=0\)</span>. In other words, say we estimate using OLS some effect, <span class="math inline">\(\hat{\beta}_{X, W_{1}}\)</span>. How big must the role of selection on unobservables be in order to completely overpower our estimate such that the true effect is actually 0?</p>
<p>Another approach is to consider a range of <span class="math inline">\(R^{2}_{max}\)</span> and <span class="math inline">\(\delta\)</span> to bound the estimated treatment effect. Using <span class="math inline">\(\delta=1\)</span> as an upper bound for <span class="math inline">\(\delta\)</span> (i.e., observables are at least as important as the unobservables), and <span class="math inline">\(\bar{R}^{2}_{max}\)</span> as an upper bound for <span class="math inline">\(R^{2}_{max}\)</span>, then the bounds on <span class="math inline">\(\beta^{*}\)</span> are <span class="math inline">\(\left[ \hat{\beta}_{X,W_{1}}, \beta^{*}(\bar{R}^{2}_{max}, 1) \right]\)</span>.</p>
<p>Finally, <span class="citation">Oster (2019)</span> suggests setting <span class="math inline">\(\delta=1\)</span> and identifying the value of <span class="math inline">\(R^{2}_{max}\)</span> for which <span class="math inline">\(\beta=0\)</span>. This would tell us how much of the variation in <span class="math inline">\(Y\)</span> would need to be explained by unobservables in order for the true effect to be null (given our estimate, <span class="math inline">\(\hat{\beta}_{X,W_{1}}\)</span>.</p>
<p>There is also a Stata command, <code>psacalc</code>, to do these calculations for us!<br />
# References</p>
<div id="refs" class="references">
<div id="ref-altonji2005">
<p>Altonji, Joseph G, Todd E Elder, and Christopher R Taber. 2005. “An Evaluation of Instrumental Variable Strategies for Estimating the Effects of Catholic Schooling.” <em>Journal of Human Resources</em> 40 (4): 791–821.</p>
</div>
<div id="ref-oster2019">
<p>Oster, Emily. 2019. “Unobservable Selection and Coefficient Stability: Theory and Evidence.” <em>Journal of Business &amp; Economic Statistics</em> 37 (2): 187–204. <a href="https://doi.org/10.1080/07350015.2016.1227711">https://doi.org/10.1080/07350015.2016.1227711</a>.</p>
</div>
</div>
